{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 0\n",
    "# Each one of the datasets has properties which makes\n",
    "# them hard to learn. Motivate which of the three problems is most\n",
    "# difficult for a decision tree algorithm to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MONK-2 is the harderst to learn. In order to classify a sample, you\n",
    "# always have to check values of all 6 params. It is only\n",
    "# possible to split the space based on all 6 parameter values.\n",
    "# It is hard to compute the information gain and choose the split\n",
    "# parameter, the output/classification of a sample\n",
    "# changes depending on the value of the remaining parameters\n",
    "\n",
    "# MONK-1 - It is hard to choose a parameter to split between a1 and a2.\n",
    "# Would require a split on all values of a1 and then a split on\n",
    "# values of a2 (2 + 3 + 3^2 branches)\n",
    "\n",
    "# MONK-3 has the lowest number of training samples\n",
    "# and contains additional noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 1\n",
    "# The file dtree.py defines a function entropy which\n",
    "# calculates the entropy of a dataset. Import this file along with the\n",
    "# monks datasets and use it to calculate the entropy of the training\n",
    "# datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../dectrees/python')\n",
    "import monkdata as m\n",
    "import dtree as dt\n",
    "import drawtree_qt5 as qt\n",
    "import statistics\n",
    "import pyqtgraph as pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of MONK-1: 1.0\n",
      "Entropy of MONK-2: 0.957117428264771\n",
      "Entropy of MONK-3: 0.9998061328047111\n"
     ]
    }
   ],
   "source": [
    "monks = [m.monk1, m.monk2, m.monk3]\n",
    "\n",
    "for idx,monk in enumerate(monks):\n",
    "    print(\"Entropy of MONK-\" + str(idx+1) + \": \" + str(dt.entropy(monk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 2\n",
    "# Explain entropy for a uniform distribution and a\n",
    "# non-uniform distribution, present some example distributions with\n",
    "# high and low entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy(S) = -Sig(p_{i} * log(p_{i})) - measure of disorder, impurity\n",
    "# of a dataset. Maximized when you're least likely to predict the\n",
    "# outcome/class of a sample. Logarithm(p_{i}) = number of splits/bits\n",
    "# to classify correctly/encode correctly\n",
    "\n",
    "# In a Uniform Distribution every element of the sample space is\n",
    "# equally likely to occur/be drawn.\n",
    "# Entropy is a measure of uncertainty. How uncertain are you about the\n",
    "# category/outcome of a sample you would draw from a given dataset\n",
    "\n",
    "# 2 red and 2 black cards - 2 possibilities, equally likely.\n",
    "# As uncertain as you can be about the outcome of a draw -> max Entropy\n",
    "# E(S) = -1/2 * log(1/2) - 1/2 * log(1/2) = 1\n",
    "\n",
    "# In a non-unform distribution some elements are more likely to\n",
    "# occur than others. If you make a random guess/draw, you are more likely\n",
    "# to get one category over the others. \n",
    "\n",
    "# 4 cards - 3 black, one red, in a random draw 75% that you pick a\n",
    "# black one, 25% red one. Not so uncertain and thus the entropy is lower\n",
    "# -3/4 * log(3/4) - 1/4 * log(1/4) = 0.24\n",
    "# (show the decrease against 2 red to 2 black)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 3\n",
    "# Use the function averageGain (defined in dtree.py)\n",
    "# to calculate the expected information gain corresponding to each of\n",
    "# the six attributes. Note that the attributes are represented as\n",
    "# instances of the class Attribute (defined in monkdata.py) which you\n",
    "# can access via m.attributes[0], ..., m.attributes[5]. Based on\n",
    "# the results, which attribute should be used for splitting\n",
    "# the examples at the root node?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         A1    A2    A3    A4    A5    A6    \n",
      "MONK-1 0.075 0.006 0.005 0.026 0.287 0.001 \n",
      "MONK-2 0.004 0.002 0.001 0.016 0.017 0.006 \n",
      "MONK-3 0.007 0.294 0.001 0.003 0.256 0.007 \n"
     ]
    }
   ],
   "source": [
    "print(\"         \", end='')\n",
    "for attribute in m.attributes:\n",
    "    print(str(attribute) + \"    \", end='')\n",
    "print()\n",
    "\n",
    "for idx, monk in enumerate(monks):\n",
    "    print(\"MONK-\" + str(idx+1) + \" \", end='')\n",
    "    for attribute in m.attributes:\n",
    "        print(\"%.3f\" % dt.averageGain(monk,attribute) + \" \", end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best-split attributes\n",
    "# MONK-1: A5\n",
    "# MONK-2: A5\n",
    "# MONK-3: A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 4\n",
    "# For splitting we choose the attribute that maximizes\n",
    "# the information gain, Eq.3. Looking at Eq.3 how does the entropy of\n",
    "# the subsets, S k , look like when the information gain is maximized?\n",
    "# How can we motivate using the information gain as a heuristic for\n",
    "# picking an attribute for splitting? Think about reduction in entropy\n",
    "# after the split and what the entropy implies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we split a dataset on a parameter that maximizes the information\n",
    "# gain, we minimize the entropy of the new groups. The goal is to\n",
    "# minimize the entropy/impurity - to order the set.\n",
    "# Therefore infromation gain as a heuristic for choosing split\n",
    "# parameters exactly fits into the idea of ordering of the dataset\n",
    "# into groups of samples that belong together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the MONK-1 data draw the decision tree up to the first two levels and\n",
    "# assign the majority class of the subsets that resulted from the two splits\n",
    "# to the leaf nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A5(+A4(---)A6(--)A1(--+))\n"
     ]
    }
   ],
   "source": [
    "print(dt.buildTree(m.monk1, m.attributes, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A5+A4---A6--A1--+"
     ]
    }
   ],
   "source": [
    "def buildTreeCustom(dataset, depth):\n",
    "    if (depth > 0):\n",
    "        bestAttr = dt.bestAttribute(dataset, m.attributes)\n",
    "        print(str(bestAttr), end='')\n",
    "        \n",
    "        # Select datasets splits for each value of the bestAttr\n",
    "        splits = []\n",
    "        for value in bestAttr.values:\n",
    "            splits.append(dt.select(dataset, bestAttr, value))\n",
    "                \n",
    "        for split in splits:\n",
    "            # If entropy of the split > 0, the split is impure and we can further split it. Recursive call with reduced depth\n",
    "            if (dt.entropy(split) > 0):\n",
    "                buildTreeCustom(split, depth-1)\n",
    "            else:\n",
    "                print('+' if dt.mostCommon(split) else '-', end='')\n",
    "    else:\n",
    "        print('+' if dt.mostCommon(dataset) else '-', end='')\n",
    "\n",
    "buildTreeCustom(m.monk1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 5\n",
    "# Build the full decision trees for all three Monk datasets using\n",
    "# buildTree. Then, use the function check to measure the performance of the decision tree on both the training and\n",
    "# test datasets. \n",
    "# Compute the train and test set errors for the three Monk datasets\n",
    "# for the full trees. Were your assumptions about the datasets correct?\n",
    "# Explain the results you get for the training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONK-1 Training set: 1.0, Test set: 0.8287037037037037\n",
      "MONK-2 Training set: 1.0, Test set: 0.6921296296296297\n",
      "MONK-3 Training set: 1.0, Test set: 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "t1 = dt.buildTree(m.monk1, m.attributes)\n",
    "t2 = dt.buildTree(m.monk2, m.attributes)\n",
    "t3 = dt.buildTree(m.monk3, m.attributes)\n",
    "\n",
    "print(\"MONK-1 Training set: \" + str(dt.check(t1, m.monk1)) + \", Test set: \" + str(dt.check(t1, m.monk1test)))\n",
    "print(\"MONK-2 Training set: \" + str(dt.check(t2, m.monk2)) + \", Test set: \" + str(dt.check(t2, m.monk2test)))\n",
    "print(\"MONK-3 Training set: \" + str(dt.check(t3, m.monk3)) + \", Test set: \" + str(dt.check(t3, m.monk3test)))\n",
    "\n",
    "# Assumptions seem to be correct. All samples in the training datasets\n",
    "# have been correctly classified. In the test sets respectively\n",
    "# 83%, 69% and 94% correctly classified (MONK-2 the hardest to learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def partition(data, fraction):\n",
    "    ldata = list(data)\n",
    "    random.shuffle(ldata)\n",
    "    breakPoint = int(len(ldata) * fraction)\n",
    "    return ldata[:breakPoint], ldata[breakPoint:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code which performs the complete pruning by repeatedly calling\n",
    "# allPruned and picking the tree which gives the best classification\n",
    "# performance on the validation dataset. You should stop pruning when\n",
    "# all the pruned trees perform worse than the current candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune(tree, valSet):\n",
    "    currentTree = tree\n",
    "    currentPerf = dt.check(currentTree, valSet)\n",
    "    pTrees = dt.allPruned(currentTree)\n",
    "    for pTree in pTrees:\n",
    "        if (dt.check(pTree, valSet) > currentPerf):\n",
    "            currentTree = prune(pTree, valSet)\n",
    "            currentPerf = dt.check(currentTree, valSet)\n",
    "    return currentTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "monk1train, monk1val = partition(m.monk1, 0.6)\n",
    "qt.drawTree(prune(t1, monk1val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 6\n",
    "# Explain pruning from a bias variance trade-off perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The more levels in a tree, the more accurate classification\n",
    "# Thus, if we go deeper in a tree, we reduce the bias\n",
    "# (complex model fits the training data better) but we increase the \n",
    "# variance (even a small change in training data, will result in a\n",
    "# big change in the tree).\n",
    "# Pruning increases bias as samples are not so precisely classified\n",
    "# anymore due to the majority class calssification of subtrees but\n",
    "# at the same time reduces variance (classification based on\n",
    "# majority class is not likely to change due to an unsignificant\n",
    "# change in the training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 7\n",
    "# Evaluate the effect pruning has on the test error for\n",
    "# the monk1 and monk3 datasets, in particular determine the optimal\n",
    "# partition into training and pruning by optimizing the parameter\n",
    "# fraction. Plot the classification error on the test sets as a\n",
    "# function of the parameter fraction âˆˆ {0.3, 0.4, 0.5, 0.6, 0.7, 0.8}.\n",
    "# Note that the split of the data is random. We therefore need to\n",
    "# compute the statistics over several runs of the split to be able to\n",
    "# draw any conclusions. Reasonable statistics includes mean and a\n",
    "# measure of the spread. Do remember to print axes labels, legends\n",
    "# and data points as you will not pass without them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fractions = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "m1performance = []\n",
    "m3performance = []\n",
    "\n",
    "for fraction in fractions:\n",
    "    val1 = 0\n",
    "    val3 = 0\n",
    "\n",
    "    for i in range(100):\n",
    "        monk1train, monk1val = partition(m.monk1, fraction)\n",
    "        t1 = dt.buildTree(monk1train, m.attributes)\n",
    "        val1 += dt.check(prune(t1, monk1val), m.monk1test)\n",
    "\n",
    "        monk3train, monk3val = partition(m.monk3, fraction)\n",
    "        t3 = dt.buildTree(monk3train, m.attributes)\n",
    "        val3 += dt.check(prune(t3, monk3val), m.monk3test)\n",
    "\n",
    "    val1 /= 100\n",
    "    m1performance.append(val1)\n",
    "\n",
    "    val3 /= 100\n",
    "    m3performance.append(val3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1error = []\n",
    "m3error = []\n",
    "for i in range (0,6):\n",
    "    m1error.append(1-m1performance[i])\n",
    "    m3error.append(1-m3performance[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean error for pruned trees for different validation sets\n",
      "  0.3    0.4    0.5    0.6    0.7    0.8    \n",
      "0.2205 0.2051 0.1797 0.1663 0.1461 0.1355 \n",
      "0.0791 0.0608 0.0460 0.0458 0.0432 0.0521 \n",
      "\n",
      "Mean and Variance of error for pruned trees\n",
      "MONK-1 Mean error: 0.1755 Variance: 0.0011\n",
      "MONK-3 Mean error: 0.0545 Variance: 0.0002\n",
      "\n",
      "Error for unpruned trees\n",
      "MONK-1: 0.1574\n",
      "MONK-3: 0.0602\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean error for pruned trees for different validation sets\")\n",
    "print(\"  \", end='')\n",
    "\n",
    "for fraction in fractions:\n",
    "    print(str(fraction) + \"    \", end='')\n",
    "print()\n",
    "\n",
    "for error in m1error:\n",
    "    print(\"%.4f\" %  error + \" \", end='')\n",
    "print() \n",
    "    \n",
    "for error in m3error:\n",
    "    print(\"%.4f\" %  error + \" \", end='')\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"Mean and Variance of error for pruned trees\")\n",
    "print(\"MONK-1 Mean error: \" + str(\"%.4f\" % statistics.mean(m1error)) + \" Variance: \" + str(\"%.4f\" % statistics.variance(m1error)))\n",
    "print(\"MONK-3 Mean error: \" + str(\"%.4f\" % statistics.mean(m3error)) + \" Variance: \" + str(\"%.4f\" % statistics.variance(m3error)))\n",
    "print()\n",
    "\n",
    "print(\"Error for unpruned trees\")\n",
    "print(\"MONK-1: \" + str(\"%.4f\" % (1-dt.check(t1, m.monk1test))))\n",
    "print(\"MONK-3: \" + str(\"%.4f\" % (1-dt.check(t3, m.monk3test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "app = pg.mkQApp()\n",
    "\n",
    "plt = pg.plot(fractions, m1error, title='Mean error for MONK-1', symbol='o', pen=None, left='Classification error', bottom='Fraction of the training set used for training')\n",
    "plt.showGrid(x=True,y=True)\n",
    "\n",
    "plt2 = pg.plot(fractions, m3error, title='Mean error for MONK-3', symbol='o', pen=None, left='Classification error', bottom='Fraction of the training set used for training')\n",
    "plt2.showGrid(x=True,y=True)\n",
    "\n",
    "status = app.exec_()\n",
    "sys.exit(status)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 0\n",
    "# Each one of the datasets has properties which makes\n",
    "# them hard to learn. Motivate which of the three problems is most\n",
    "# difficult for a decision tree algorithm to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MONK-1 it is hard to choose a parameter to split between a1 and a2.\n",
    "# Would require a split on all values of a1 and then a split on values of a2 (2 + 3 + 3^2 branches)\n",
    "\n",
    "# MONK-3 the trickiest. In order to evaluate if the statement holds, you always have to check all 6 params.\n",
    "# It is only possible to split the space based on all 6 parameter values. It is hard to compute the information gain\n",
    "# and choose the split parameter, the output(if condition holds or not +/-) of a sample changes depending on the value\n",
    "# of the remaining parameters\n",
    "\n",
    "# MONK-3 has the lowest number of training samples and contains additional noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 1\n",
    "# The file dtree.py defines a function entropy which\n",
    "# calculates the entropy of a dataset. Import this file along with the\n",
    "# monks datasets and use it to calculate the entropy of the training\n",
    "# datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../dectrees/python')\n",
    "import monkdata as m\n",
    "import dtree as dt\n",
    "import drawtree_qt5 as qt\n",
    "import statistics\n",
    "import pyqtgraph as pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of monk1: 1.0\n",
      "Entropy of monk2: 0.957117428264771\n",
      "Entropy of monk3: 0.9998061328047111\n"
     ]
    }
   ],
   "source": [
    "monks = [m.monk1, m.monk2, m.monk3]\n",
    "\n",
    "for idx,monk in enumerate(monks):\n",
    "    print(\"Entropy of monk\" + str(idx+1) + \": \" + str(dt.entropy(monk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 2\n",
    "# Explain entropy for a uniform distribution and a\n",
    "# non-uniform distribution, present some example distributions with\n",
    "# high and low entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a Uniform Distribution every element of the sample space is\n",
    "# equally likely to occur/be drawn.\n",
    "# Entropy is a measure of uncertainty. How uncertain are you about the\n",
    "# category/outcome of a sample you would draw from a given dataset\n",
    "\n",
    "# Coin toss/ 2 cards - 2 possibilities, equally likely.\n",
    "# As uncertain as you can be about the outcome of a toss -> max Entropy\n",
    "\n",
    "# In a non-unform distribution some elements are more likely to\n",
    "# occur than others. If you make a random guess/draw, you are more likely\n",
    "# to get one category over the others. \n",
    "\n",
    "# 3 cards - 2 black one red, in a random draw 67% that you pick a black one\n",
    "# 33% red one. Not so uncertain and thus entropy is lower\n",
    "# -2/3 * log(2/3) - 1/3 * log(1/3) = 0.28\n",
    "# -3/4 * log(3/4) - 1/4 * log(1/4) = 0.24\n",
    "# (to show the decrease against 2 red to black)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 3\n",
    "# Use the function averageGain (defined in dtree.py)\n",
    "# to calculate the expected information gain corresponding to each of\n",
    "# the six attributes. Note that the attributes are represented as\n",
    "# instances of the class Attribute (defined in monkdata.py) which you\n",
    "# can access via m.attributes[0], ..., m.attributes[5]. Based on\n",
    "# the results, which attribute should be used for splitting\n",
    "# the examples at the root node?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         A1    A2    A3    A4    A5    A6    \n",
      "MONK-1 0.075 0.006 0.005 0.026 0.287 0.001 \n",
      "MONK-2 0.004 0.002 0.001 0.016 0.017 0.006 \n",
      "MONK-3 0.007 0.294 0.001 0.003 0.256 0.007 \n"
     ]
    }
   ],
   "source": [
    "print(\"         \", end='')\n",
    "for attribute in m.attributes:\n",
    "    print(str(attribute) + \"    \", end='')\n",
    "print()\n",
    "\n",
    "for idx, monk in enumerate(monks):\n",
    "    print(\"MONK-\" + str(idx+1) + \" \", end='')\n",
    "    for attribute in m.attributes:\n",
    "        print(\"%.3f\" % dt.averageGain(monk,attribute) + \" \", end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best-split attributes\n",
    "# MONK-1: A5\n",
    "# MONK-2: A5\n",
    "# MONK-3: A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 4\n",
    "# For splitting we choose the attribute that maximizes\n",
    "# the information gain, Eq.3. Looking at Eq.3 how does the entropy of\n",
    "# the subsets, S k , look like when the information gain is maximized?\n",
    "# How can we motivate using the information gain as a heuristic for\n",
    "# picking an attribute for splitting? Think about reduction in entropy\n",
    "# after the split and what the entropy implies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we split a dataset on a parameter that maximizes the information gain, \n",
    "# we minimize the entropy of the dataset. The goal is to minimize\n",
    "# the entropy of a dataset, reduce the impurity/order the set.\n",
    "# Therefore infromation gain as a heuristic for choosing split\n",
    "# parameters exactly fits into the idea of ordering of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the monk1 data draw the decision tree up to the first two levels and\n",
    "# assign the majority class of the subsets that resulted from the two splits\n",
    "# to the leaf nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A5(+A4(---)A6(--)A1(--+))\n"
     ]
    }
   ],
   "source": [
    "print(dt.buildTree(m.monk1, m.attributes, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A5+A4---A6--A1--+"
     ]
    }
   ],
   "source": [
    "def buildTreeCustom(dataset, depth):\n",
    "    if (depth > 0):\n",
    "        bestAttr = dt.bestAttribute(dataset, m.attributes)\n",
    "        print(str(bestAttr), end='')\n",
    "        \n",
    "        # Select datasets splits for each value of the bestAttr\n",
    "        splits = []\n",
    "        for value in bestAttr.values:\n",
    "            splits.append(dt.select(dataset, bestAttr, value))\n",
    "                \n",
    "        for split in splits:\n",
    "            # If entropy of the split > 0, the split is impure and we can further split it. Recursive call with reduced depth\n",
    "            if (dt.entropy(split) > 0):\n",
    "                buildTreeCustom(split, depth-1)\n",
    "            else:\n",
    "                print('+' if dt.mostCommon(split) else '-', end='')\n",
    "    else:\n",
    "        print('+' if dt.mostCommon(dataset) else '-', end='')\n",
    "\n",
    "buildTreeCustom(m.monk1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 5\n",
    "# Build the full decision trees for all three Monk datasets using\n",
    "# buildTree. Then, use the function check to measure the performance of the decision tree on both the training and\n",
    "# test datasets. \n",
    "# Compute the train and test set errors for the three Monk datasets\n",
    "# for the full trees. Were your assumptions about the datasets correct?\n",
    "# Explain the results you get for the training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONK-1 Training set: 1.0, Test set: 0.8287037037037037\n",
      "MONK-2 Training set: 1.0, Test set: 0.6921296296296297\n",
      "MONK-3 Training set: 1.0, Test set: 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "t1 = dt.buildTree(m.monk1, m.attributes)\n",
    "t2 = dt.buildTree(m.monk2, m.attributes)\n",
    "t3 = dt.buildTree(m.monk3, m.attributes)\n",
    "\n",
    "print(\"MONK-1 Training set: \" + str(dt.check(t1, m.monk1)) + \", Test set: \" + str(dt.check(t1, m.monk1test)))\n",
    "print(\"MONK-2 Training set: \" + str(dt.check(t2, m.monk2)) + \", Test set: \" + str(dt.check(t2, m.monk2test)))\n",
    "print(\"MONK-3 Training set: \" + str(dt.check(t3, m.monk3)) + \", Test set: \" + str(dt.check(t3, m.monk3test)))\n",
    "\n",
    "# Assumptions seem to be correct. All samples in the training datasets have been correctly classified.\n",
    "# In the test sets respectively 83%, 69% and 94% correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def partition(data, fraction):\n",
    "    ldata = list(data)\n",
    "    random.shuffle(ldata)\n",
    "    breakPoint = int(len(ldata) * fraction)\n",
    "    return ldata[:breakPoint], ldata[breakPoint:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code which performs the complete pruning by repeatedly calling\n",
    "# allPruned and picking the tree which gives the best classification\n",
    "# performance on the validation dataset. You should stop pruning when\n",
    "# all the pruned trees perform worse than the current candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune(tree, valSet):\n",
    "    currentTree = tree\n",
    "    currentPerf = dt.check(currentTree, valSet)\n",
    "    pTrees = dt.allPruned(currentTree)\n",
    "    for pTree in pTrees:\n",
    "        if (dt.check(pTree, valSet) > currentPerf):\n",
    "            currentTree = prune(pTree, valSet)\n",
    "            currentPerf = dt.check(currentTree, valSet)\n",
    "    return currentTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "monk1train, monk1val = partition(m.monk1, 0.6)\n",
    "qt.drawTree(prune(t1, monk1val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 6\n",
    "# Explain pruning from a bias variance trade-off perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The more levels in a tree, the more accurate classification\n",
    "# Thus, if we go deeper in a tree, we reduce the bias but increase\n",
    "# variance (even a small change in training data, will result in a\n",
    "# big change in the tree).\n",
    "# Pruning increases bias as samples are not so precisely classified\n",
    "# anymore due to the majority class of subtree but at the same time\n",
    "# reduces variance. Outcome based on majority class not likely to\n",
    "# change unless a more significant change in the training data occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 7\n",
    "# Evaluate the effect pruning has on the test error for\n",
    "# the monk1 and monk3 datasets, in particular determine the optimal\n",
    "# partition into training and pruning by optimizing the parameter\n",
    "# fraction. Plot the classification error on the test sets as a\n",
    "# function of the parameter fraction ∈ {0.3, 0.4, 0.5, 0.6, 0.7, 0.8}.\n",
    "# Note that the split of the data is random. We therefore need to\n",
    "# compute the statistics over several runs of the split to be able to\n",
    "# draw any conclusions. Reasonable statistics includes mean and a\n",
    "# measure of the spread. Do remember to print axes labels, legends\n",
    "# and data points as you will not pass without them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "fractions = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "m1performance = []\n",
    "m3performance = []\n",
    "\n",
    "for fraction in fractions:\n",
    "    val1 = 0\n",
    "    val3 = 0\n",
    "\n",
    "    for i in range(100):\n",
    "        monk1train, monk1val = partition(m.monk1, fraction)\n",
    "        t1 = dt.buildTree(monk1train, m.attributes)\n",
    "        val1 += dt.check(prune(t1, monk1val), m.monk1test)\n",
    "\n",
    "        monk3train, monk3val = partition(m.monk3, fraction)\n",
    "        t3 = dt.buildTree(monk3train, m.attributes)\n",
    "        val3 += dt.check(prune(t3, monk3val), m.monk3test)\n",
    "\n",
    "    val1 /= 100\n",
    "    m1performance.append(val1)\n",
    "\n",
    "    val3 /= 100\n",
    "    m3performance.append(val3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1error = []\n",
    "m3error = []\n",
    "for i in range (0,6):\n",
    "    m1error.append(1-m1performance[i])\n",
    "    m3error.append(1-m3performance[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors for pruned trees for different validation sets\n",
      "  0.3    0.4    0.5    0.6    0.7    0.8    \n",
      "0.2221 0.1991 0.1758 0.1615 0.1476 0.1359 \n",
      "0.0875 0.0611 0.0463 0.0430 0.0422 0.0494 \n",
      "\n",
      "Mean and Variance of error for pruned trees\n",
      "MONK-1 Mean error: 0.1737 Variance: 0.0010\n",
      "MONK-3 Mean error: 0.0549 Variance: 0.0003\n",
      "\n",
      "Error for unpruned trees\n",
      "MONK-1: 0.0278\n",
      "MONK-3: 0.0324\n"
     ]
    }
   ],
   "source": [
    "print(\"Errors for pruned trees for different validation sets\")\n",
    "print(\"  \", end='')\n",
    "\n",
    "for fraction in fractions:\n",
    "    print(str(fraction) + \"    \", end='')\n",
    "print()\n",
    "\n",
    "for error in m1error:\n",
    "    print(\"%.4f\" %  error + \" \", end='')\n",
    "print() \n",
    "    \n",
    "for error in m3error:\n",
    "    print(\"%.4f\" %  error + \" \", end='')\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"Mean and Variance of error for pruned trees\")\n",
    "print(\"MONK-1 Mean error: \" + str(\"%.4f\" % statistics.mean(m1error)) + \" Variance: \" + str(\"%.4f\" % statistics.variance(m1error)))\n",
    "print(\"MONK-3 Mean error: \" + str(\"%.4f\" % statistics.mean(m3error)) + \" Variance: \" + str(\"%.4f\" % statistics.variance(m3error)))\n",
    "print()\n",
    "\n",
    "print(\"Error for unpruned trees\")\n",
    "print(\"MONK-1: \" + str(\"%.4f\" % (1-dt.check(t1, m.monk1test))))\n",
    "print(\"MONK-3: \" + str(\"%.4f\" % (1-dt.check(t3, m.monk3test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "app = pg.mkQApp()\n",
    "\n",
    "plt = pg.plot(fractions, m1error, title='Mean error for MONK-1', symbol='o', pen=None, left='Classification error', bottom='Fraction of the training set used for training')\n",
    "plt.showGrid(x=True,y=True)\n",
    "\n",
    "plt2 = pg.plot(fractions, m3error, title='Mean error for MONK-3', symbol='o', pen=None, left='Classification error', bottom='Fraction of the training set used for training')\n",
    "plt2.showGrid(x=True,y=True)\n",
    "\n",
    "status = app.exec_()\n",
    "sys.exit(status)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
